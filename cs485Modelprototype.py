# -*- coding: utf-8 -*-
"""CS485ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z98lm5V7_SQWSBKJF6510cq8LSGGdS6u
"""

import tensorflow as tf
from tensorflow import keras

import os
import tempfile
import ipaddress

import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

import sklearn
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler,MinMaxScaler

mpl.rcParams['figure.figsize'] = (12, 10)
colors = plt.rcParams['axes.prop_cycle'].by_key()['color']

raw_df = pd.read_csv('/content/combined_output.csv')
raw_df.head()

df = raw_df.copy()
df['@timestamp'] = pd.to_datetime(df['@timestamp'].str.replace(" @ ", " "), format="%b %d, %Y %H:%M:%S.%f")

# Convert to numeric format (seconds since epoch)
df['Numeric_Timestamp'] = df['@timestamp'].apply(lambda x: x.timestamp())

# Convert to numeric format (seconds since epoch)
df['Numeric_Timestamp'] = df['@timestamp'].apply(lambda x: x.timestamp())
df.head()

print("Columns in DataFrame:", df.columns.tolist())

eps = 0.001 # 0 => 0.1Â¢
df['Log_Numeric_Timestamp'] = np.log(df.pop('Numeric_Timestamp')+eps)

df['event.outcome'] = df['event.outcome'].replace('failure', 1).replace('success', 0).replace('unknown',2)

df['Class'] = df['Class'].replace('Good', 0).replace('Bad', 1).replace('unknown',2)

df['client.geo.location.lat'] = df['client.geo.location.lat'].str.encode('utf-8')
df['client.geo.location.lon'] = df['client.geo.location.lon'].str.encode('utf-8')
df['client.ip'] = df['client.ip'].str.encode('utf-8')

df['client.geo.location.lat'] = pd.to_numeric(df['client.geo.location.lat'],errors='coerce')
df['client.geo.location.lon'] = pd.to_numeric(df['client.geo.location.lon'],errors='coerce')
df.head()

def ip_to_int(ip):
    # Decode if it's in byte format
    if isinstance(ip, bytes):
        ip = ip.decode('utf-8')

    # Check if the IP is valid
    try:
        return int(ipaddress.ip_address(ip))
    except ValueError:
        return None

df['client.ip_as_int'] = df['client.ip'].apply(ip_to_int)

columns_to_normalize = ['Log_Numeric_Timestamp', 'client.geo.location.lat', 'client.geo.location.lon','client.ip_as_int', 'event.outcome']
min_max_scaler = MinMaxScaler()
df[columns_to_normalize] = min_max_scaler.fit_transform(df[columns_to_normalize])

columns_to_keep = ['Log_Numeric_Timestamp', 'client.geo.location.lat', 'client.geo.location.lon','client.ip_as_int', 'event.outcome','Class']
df_clean = df[columns_to_keep]
df_clean.head()

df_clean['Log_Numeric_Timestamp'].replace('None', np.nan, inplace=True)
df_clean['client.geo.location.lat'].replace('None', np.nan, inplace=True)
df_clean['client.geo.location.lat'].replace('None', np.nan, inplace=True)
df_clean['client.ip_as_int'].replace('None', np.nan, inplace=True)
df_clean['event.outcome'].replace('None', np.nan, inplace=True)

df_clean['Log_Numeric_Timestamp'] = pd.to_numeric(df['Log_Numeric_Timestamp'], errors='coerce')
df_clean['client.geo.location.lat'] = pd.to_numeric(df['client.geo.location.lat'], errors='coerce')
df_clean['client.geo.location.lon'] = pd.to_numeric(df['client.geo.location.lon'], errors='coerce')
df_clean['client.ip_as_int'] = pd.to_numeric(df['client.ip_as_int'], errors='coerce')
df_clean['event.outcome'] = pd.to_numeric(df['event.outcome'], errors='coerce')

# Use a utility from sklearn to split and shuffle your dataset.
train_df, test_df = train_test_split(df_clean, test_size=0.2)
train_df, val_df = train_test_split(train_df, test_size=0.2)

# Form np arrays of labels and features.
train_labels = np.array(train_df.pop('Class')).reshape(-1, 1)
bool_train_labels = train_labels[:, 0] != 0
val_labels = np.array(val_df.pop('Class')).reshape(-1, 1)
test_labels = np.array(test_df.pop('Class')).reshape(-1, 1)

train_features = np.array(train_df)
val_features = np.array(val_df)
test_features = np.array(test_df)

scaler = StandardScaler()
train_features = scaler.fit_transform(train_features)

val_features = scaler.transform(val_features)
test_features = scaler.transform(test_features)

train_features = np.clip(train_features, -5, 5)
val_features = np.clip(val_features, -5, 5)
test_features = np.clip(test_features, -5, 5)


print('Training labels shape:', train_labels.shape)
print('Validation labels shape:', val_labels.shape)
print('Test labels shape:', test_labels.shape)

print('Training features shape:', train_features.shape)
print('Validation features shape:', val_features.shape)
print('Test features shape:', test_features.shape)

METRICS = [
      keras.metrics.BinaryCrossentropy(name='cross entropy'),  # same as model's loss
      keras.metrics.MeanSquaredError(name='Brier score'),
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'),
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
      keras.metrics.Recall(name='recall'),
      keras.metrics.AUC(name='auc'),
      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve
]

def make_model(metrics=METRICS, output_bias=None):
  if output_bias is not None:
    output_bias = tf.keras.initializers.Constant(output_bias)
  model = keras.Sequential([
      keras.layers.Dense(
          16, activation='relu',
          input_shape=(train_features.shape[-1],)),
      keras.layers.Dropout(0.5),
      keras.layers.Dense(1, activation='sigmoid',
                         bias_initializer=output_bias),
  ])

  model.compile(
      optimizer=keras.optimizers.Adam(learning_rate=1e-3),
      loss=keras.losses.BinaryCrossentropy(),
      metrics=metrics)

  return model

EPOCHS = 100
BATCH_SIZE = 2048

def early_stopping():
 return tf.keras.callbacks.EarlyStopping(
    monitor='val_prc',
    verbose=1,
    patience=10,
    mode='max',
    restore_best_weights=True)

model = make_model()
model.summary()

model.predict(train_features[:10])

initial_weights = os.path.join(tempfile.mkdtemp(), 'initial.weights.h5')
model.save_weights(initial_weights)

print(df_clean.dtypes)

model = make_model()
model.load_weights(initial_weights)
baseline_history = model.fit(
    train_features,
    train_labels,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    callbacks=[early_stopping()],
    validation_data=(val_features, val_labels))
best_weights = os.path.join(tempfile.mkdtemp(), 'initial.weights.h5')
model.save_weights(best_weights)

initial_weights = os.path.join(tempfile.mkdtemp(), 'initial.weights.h5')
model.save_weights(initial_weights)

model = make_model()
model.load_weights(initial_weights)
model.layers[-1].bias.assign([0.0])
zero_bias_history = model.fit(
    train_features,
    train_labels,
    batch_size=BATCH_SIZE,
    epochs=20,
    validation_data=(val_features, val_labels),
    verbose=0)

model = make_model()
model.load_weights(initial_weights)
careful_bias_history = model.fit(
    train_features,
    train_labels,
    batch_size=BATCH_SIZE,
    epochs=20,
    validation_data=(val_features, val_labels),
    verbose=0)

def plot_loss(history, label, n):
  # Use a log scale on y-axis to show the wide range of values.
  plt.semilogy(history.epoch, history.history['loss'],
               color=colors[n], label='Train ' + label)
  plt.semilogy(history.epoch, history.history['val_loss'],
               color=colors[n], label='Val ' + label,
               linestyle="--")
  plt.xlabel('Epoch')
  plt.ylabel('Loss')
  plt.legend()

plot_loss(zero_bias_history, "Zero Bias", 0)
plot_loss(careful_bias_history, "Careful Bias", 1)

model = make_model()
model.load_weights(initial_weights)
baseline_history = model.fit(
    train_features,
    train_labels,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    callbacks=[early_stopping()],
    validation_data=(val_features, val_labels))

def plot_metrics(history):
  metrics = ['loss', 'prc', 'precision', 'recall']
  for n, metric in enumerate(metrics):
    name = metric.replace("_"," ").capitalize()
    plt.subplot(2,2,n+1)
    plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')
    plt.plot(history.epoch, history.history['val_'+metric],
             color=colors[0], linestyle="--", label='Val')
    plt.xlabel('Epoch')
    plt.ylabel(name)
    if metric == 'loss':
      plt.ylim([0, plt.ylim()[1]])
    elif metric == 'auc':
      plt.ylim([0.8,1])
    else:
      plt.ylim([0,1])

    plt.legend()

plot_metrics(baseline_history)

train_predictions_baseline = model.predict(train_features, batch_size=BATCH_SIZE)
test_predictions_baseline = model.predict(test_features, batch_size=BATCH_SIZE)

def plot_cm(labels, predictions, threshold=0.5):
  cm = confusion_matrix(labels, predictions > threshold)
  plt.figure(figsize=(5,5))
  sns.heatmap(cm, annot=True, fmt="d")
  plt.title('Confusion matrix @{:.2f}'.format(threshold))
  plt.ylabel('Actual label')
  plt.xlabel('Predicted label')

  print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])
  print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])
  print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])
  print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])
  print('Total Fraudulent Transactions: ', np.sum(cm[1]))

baseline_results = model.evaluate(test_features, test_labels,
                                  batch_size=BATCH_SIZE, verbose=0)
for name, value in zip(model.metrics_names, baseline_results):
  print(name, ': ', value)
print()

plot_cm(test_labels, test_predictions_baseline)

plot_cm(test_labels, test_predictions_baseline, threshold=0.1)
plot_cm(test_labels, test_predictions_baseline, threshold=0.01)

predictions = model.predict(test_features)
print(predictions)

model.save("model_prototype1.h5")

